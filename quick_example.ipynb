{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ucimlrepo pandas numpy scikit-learn openpyxl requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da25613",
   "metadata": {},
   "source": [
    "### Download and import UC-Irvine online retail dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Online Retail dataset from UCI repository using requests\n",
    "# (requests handles SSL better than urllib)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"\n",
    "\n",
    "print(\"Downloading Online Retail dataset from UCI repository...\")\n",
    "try:\n",
    "    # Use requests library which handles SSL certificates better\n",
    "    response = requests.get(url, verify=False)  # verify=False bypasses SSL verification\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Read the Excel file from the downloaded content\n",
    "    df = pd.read_excel(BytesIO(response.content), engine='openpyxl')\n",
    "    print(\"✓ Dataset loaded successfully!\")\n",
    "    \n",
    "    # Display dataset information\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Rows: {df.shape[0]:,}, Columns: {df.shape[1]}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "    print(f\"\\nData types:\")\n",
    "    # print(df.dtypes)\n",
    "    # print(f\"\\nBasic statistics:\")\n",
    "    # print(df.describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease check your internet connection or try again later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9db73",
   "metadata": {},
   "source": [
    "### Preprocess into a pricing learning table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b9f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Parse datetime\n",
    "    df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"InvoiceDate\", \"StockCode\", \"Quantity\", \"UnitPrice\"])\n",
    "\n",
    "    # Remove cancellations/returns and non-positive values\n",
    "    # UCI notes InvoiceNo starting with 'C' indicates cancellation. :contentReference[oaicite:4]{index=4}\n",
    "    df[\"InvoiceNo\"] = df[\"InvoiceNo\"].astype(str)\n",
    "    df = df[~df[\"InvoiceNo\"].str.startswith(\"C\")]\n",
    "    df = df[(df[\"Quantity\"] > 0) & (df[\"UnitPrice\"] > 0)]\n",
    "\n",
    "    # Keep only products with enough observations (helps learning stability)\n",
    "    df = df[df[\"StockCode\"].astype(str).str.len() > 0]\n",
    "\n",
    "    # Aggregate to daily product level\n",
    "    df[\"date\"] = df[\"InvoiceDate\"].dt.date\n",
    "    g = df.groupby([\"StockCode\", \"date\"], as_index=False).agg(\n",
    "        qty=(\"Quantity\", \"sum\"),\n",
    "        price=(\"UnitPrice\", \"mean\"),\n",
    "        n_txn=(\"InvoiceNo\", \"nunique\"),\n",
    "    )\n",
    "\n",
    "    # Add time features\n",
    "    g[\"date\"] = pd.to_datetime(g[\"date\"])\n",
    "    g[\"dow\"] = g[\"date\"].dt.dayofweek\n",
    "    g[\"month\"] = g[\"date\"].dt.month\n",
    "\n",
    "    # Lag features (per product)\n",
    "    g = g.sort_values([\"StockCode\", \"date\"])\n",
    "    g[\"qty_lag1\"] = g.groupby(\"StockCode\")[\"qty\"].shift(1)\n",
    "    g[\"qty_lag7\"] = g.groupby(\"StockCode\")[\"qty\"].shift(7)\n",
    "\n",
    "    # Fill missing lags with 0 (cold start)\n",
    "    g[[\"qty_lag1\", \"qty_lag7\"]] = g[[\"qty_lag1\", \"qty_lag7\"]].fillna(0)\n",
    "\n",
    "    return g\n",
    "\n",
    "data = preprocess(df)\n",
    "display(data.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['StockCode'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b84e8",
   "metadata": {},
   "source": [
    "### Fit a simple demand model from offline data:\n",
    "\n",
    "$$ log(1+qty) = \\beta_0 + \\beta_1 log(price) + context $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c826df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_demand_model(data: pd.DataFrame):\n",
    "    d = data.copy()\n",
    "    d[\"log_qty\"] = np.log1p(d[\"qty\"])\n",
    "    d[\"log_price\"] = np.log(d[\"price\"])\n",
    "\n",
    "    # identify which features are numerical and which are categorical\n",
    "    features_num = [\"log_price\", \"qty_lag1\", \"qty_lag7\", \"n_txn\"]\n",
    "    features_cat = [\"dow\", \"month\"]\n",
    "\n",
    "    X = d[features_num + features_cat]\n",
    "    y = d[\"log_qty\"]\n",
    "\n",
    "    # For numerical features, we pass them through as-is\n",
    "    # For categorical features, we apply one-hot encoding\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", \"passthrough\", features_num),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), features_cat),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Build the full pipeline with preprocessing and (Ridge)-regression model\n",
    "    model = Pipeline(steps=[\n",
    "        (\"pre\", pre),\n",
    "        # (\"reg\", Ridge(alpha=1.0)),\n",
    "        (\"reg\", LinearRegression()),\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(f\"Demand model R^2 on log_qty: {r2:.3f}\")\n",
    "    return model\n",
    "\n",
    "demand_model = fit_demand_model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023aed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display demand model coefficients\n",
    "def extract_demand_coefficients(model, data):\n",
    "    \"\"\"\n",
    "    Extract and interpret coefficients from the demand model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained sklearn Pipeline with preprocessing and regression\n",
    "        data: DataFrame used for training (to get feature names)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with coefficient names and values\n",
    "    \"\"\"\n",
    "    # Get the regression model (last step in pipeline)\n",
    "    reg_model = model.named_steps['reg']\n",
    "    \n",
    "    # Get feature names from the preprocessor\n",
    "    preprocessor = model.named_steps['pre']\n",
    "    \n",
    "    # Get feature names after transformation\n",
    "    feature_names = []\n",
    "    \n",
    "    # Numerical features (passthrough)\n",
    "    num_features = [\"log_price\", \"qty_lag1\", \"qty_lag7\", \"n_txn\"]\n",
    "    feature_names.extend(num_features)\n",
    "    \n",
    "    # Categorical features (one-hot encoded)\n",
    "    cat_transformer = preprocessor.named_transformers_['cat']\n",
    "    cat_feature_names = cat_transformer.get_feature_names_out(['dow', 'month'])\n",
    "    feature_names.extend(cat_feature_names)\n",
    "    \n",
    "    # Get coefficients\n",
    "    coefficients = reg_model.coef_\n",
    "    intercept = reg_model.intercept_\n",
    "    \n",
    "    # Create DataFrame\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': ['Intercept'] + feature_names,\n",
    "        'Coefficient': [intercept] + list(coefficients),\n",
    "        'Abs_Coefficient': [abs(intercept)] + list(np.abs(coefficients))\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute value to see most important features\n",
    "    coef_df_sorted = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DEMAND MODEL COEFFICIENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nModel: log(1 + qty) = β₀ + β₁·log(price) + β₂·qty_lag1 + ... + ε\\n\")\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(\"-\" * 70)\n",
    "    for idx, row in coef_df_sorted.head(10).iterrows():\n",
    "        feature = row['Feature']\n",
    "        coef = row['Coefficient']\n",
    "        print(f\"  {feature:25s}: {coef:8.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Price elasticity\n",
    "    price_coef = coef_df[coef_df['Feature'] == 'log_price']['Coefficient'].values[0]\n",
    "    print(f\"\\nPrice Elasticity: {price_coef:.4f}\")\n",
    "    if price_coef < 0:\n",
    "        print(f\"  → 1% increase in price → {abs(price_coef):.2f}% decrease in quantity\")\n",
    "        print(f\"  → Demand is {'elastic' if abs(price_coef) > 1 else 'inelastic'}\")\n",
    "    else:\n",
    "        print(f\"  → WARNING: Positive price coefficient (unusual!)\")\n",
    "    \n",
    "    # Lag features\n",
    "    qty_lag1_coef = coef_df[coef_df['Feature'] == 'qty_lag1']['Coefficient'].values[0]\n",
    "    qty_lag7_coef = coef_df[coef_df['Feature'] == 'qty_lag7']['Coefficient'].values[0]\n",
    "    \n",
    "    print(f\"\\nLag Effects:\")\n",
    "    print(f\"  qty_lag1: {qty_lag1_coef:7.4f} → Yesterday's demand {'increases' if qty_lag1_coef > 0 else 'decreases'} today's\")\n",
    "    print(f\"  qty_lag7: {qty_lag7_coef:7.4f} → Last week's demand {'increases' if qty_lag7_coef > 0 else 'decreases'} today's\")\n",
    "    \n",
    "    # Day of week effects\n",
    "    print(f\"\\nDay of Week Effects:\")\n",
    "    dow_features = coef_df[coef_df['Feature'].str.contains('dow_')]\n",
    "    for _, row in dow_features.iterrows():\n",
    "        day_num = row['Feature'].split('_')[-1]\n",
    "        days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        day_name = days[int(float(day_num))] if day_num.replace('.', '').isdigit() else day_num\n",
    "        print(f\"  {day_name}: {row['Coefficient']:7.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    return coef_df_sorted\n",
    "\n",
    "\n",
    "# Extract coefficients\n",
    "coef_df = extract_demand_coefficients(demand_model, data)\n",
    "\n",
    "# Display full coefficient table\n",
    "print(\"\\nFull Coefficient Table:\")\n",
    "display(coef_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e22c1",
   "metadata": {},
   "source": [
    "### Build a simple Reinforcement Learning environment (model-based \"offline RL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289155bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll define actions as price multipliers around the observed price:\n",
    "ACTION_MULTS = np.array([0.90, 0.95, 1.00, 1.05, 1.10, 1.15, 1.20, 1.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb954c",
   "metadata": {},
   "source": [
    "State will be a discretized tuple:\n",
    "- product id\n",
    "- day-of-week\n",
    "- lagged-qty bin\n",
    "\n",
    "Reward: Revenue = chosen price * simulated_qty\n",
    "\n",
    "Transition:\n",
    "- lag updates with simulated qty\n",
    "- day moves forward\n",
    "\n",
    "### Environment + Q-Learning (tabular --> simple)\n",
    "- Classic Q-Learning learns a table:\n",
    "$ Q(s,a) $ \n",
    "for every discrete state s and action a\n",
    "- State space must be finite, action space must be finite\n",
    "- Every pair (s,a) gets updated many times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f841d",
   "metadata": {},
   "source": [
    "### Notes on standard and double-Q Learning\n",
    "Standard Q-learning learns action-value function $Q(s,a)$ with update:\n",
    "\\begin{equation*}\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\bigg( r_t + \\gamma \\, max_a Q(s_{t+1}, a) - Q(s_t, a_t) \\bigg)\n",
    "\\end{equation*}\n",
    "where $\\alpha$ is a parameter that scale how much new information changes old estimate.\n",
    "\n",
    "Think of it like: $Q_{new} = (1-\\alpha) Q_{old} + \\alpha (new target)$. So:\n",
    "- Small $\\alpha$ --> learn slowly\n",
    "- Large $\\alpha$ --> learn fast but new Q-value is noisy and can be unstable\n",
    "\n",
    "$\\gamma$ is the discount factor. It weights future rewards relative to immediate reward:\n",
    "\\begin{equation*}\n",
    "Q(s,at) = E[ r_0 + \\gamma r_1 + \\gamma^2 r_2 + .... ]\n",
    "\\end{equation*}\n",
    "So $\\gamma=0$, we only care about immediate reward while $\\gamma = 1$, long term consequences matter. \n",
    "\n",
    "Intuitively, we want RL agent to keep the mindset that \"A slightly lower price today may keep demand (quantity) healthy and pay off later\"\n",
    "\n",
    "In standard Q-Learning, we expect overestimating bias:\n",
    "\\begin{equation*}\n",
    "E \\bigg[ max_a Q(s,a) \\bigg] \\geq max_a E \\bigg[ Q(s,a) \\bigg]\n",
    "\\end{equation*}\n",
    "Think of this as max() is convex. So when we take a max over noise then we tend to pick action that is overestimated (by noise).\n",
    "\n",
    "Bottom line: overestimation lead to aggressive policies (e.g., always increase price)\n",
    "\n",
    "### Introduce Double-Q Learning:\n",
    "Double-Q learning keeps 2 separate Q-value estimate:\n",
    "\\begin{align*}\n",
    "&Q^A(s,a) \\\\\n",
    "&Q^B(s,a)\n",
    "\\end{align*}\n",
    "The trick: \n",
    "- use one table to select the best action for next state (argmax)\n",
    "- use other table to evaluate that selected action\n",
    "This decouples selection and evaluation --> reducing bias from max operator\n",
    "\n",
    "Update Rules: At each transition $(s_t, a_t, r_t, s_{t+1})$, flip a coin\n",
    "- Update $Q^A$ with probability 0.5:\n",
    "    - Best action selection using $Q^A$: $a^* = argmax_a \\, Q^A(s_{t+1},a)$\n",
    "    - Evaluate that action using $Q^B$: $Y^A_t = r_t + \\gamma \\, Q^B(s_{t+1}, a^*)$\n",
    "    - Update $Q^A$:\n",
    "\\begin{equation*}\n",
    "Q^A (s_t, a_t) \\leftarrow Q^A (s_t, a_t) + \\alpha \\bigg( Y^A_t - Q^A(s_t, a_t) \\bigg)\n",
    "\\end{equation*}\n",
    "\n",
    "- Or Update $Q^B$ with probability 0.5: (using the same workflow & equations)\n",
    "\n",
    "This approach reduces overestimation because:\n",
    "- the action choosing task and evaluation task are done using 2 different Q-table\n",
    "- E.g., best action chosen from $Q^A$ while evaluation done in $Q^B$, or vice versa\n",
    "- Idea is: don't evaluate action choice using same-noise table: if noise in $Q^A$ and $Q^B$ is imperfectly correlated (they should differ because they're update differently on different steps), then the \"lucky high\" selection in $Q^A$ should not appear high in $Q^B$. Overall, leading to a less biased evaluation.\n",
    "- Analogy: use one model for choosing, another model for scoring -- this is a common bias/variance trick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_top_products(data: pd.DataFrame, top_k=200):\n",
    "    \"\"\"\n",
    "    Get the top-k products by total quantity sold.\n",
    "    \"\"\"\n",
    "    counts = data.groupby(\"StockCode\")[\"qty\"].sum().sort_values(ascending=False)\n",
    "    top = counts.head(top_k).index.astype(str).tolist()\n",
    "    return top\n",
    "\n",
    "def build_episode_table(data: pd.DataFrame, top_products):\n",
    "    \"\"\"\n",
    "    Build episode table for RL environment using only top products.\n",
    "    \"\"\"\n",
    "    d = data[data[\"StockCode\"].astype(str).isin(top_products)].copy()\n",
    "    # Use each (product, date) row as a step; we’ll “simulate” what happens next\n",
    "    d[\"StockCode\"] = d[\"StockCode\"].astype(str)\n",
    "    d = d.sort_values([\"StockCode\", \"date\"])\n",
    "    return d.reset_index(drop=True)\n",
    "\n",
    "# Discretize lagged demand into bins\n",
    "def qty_bin(q):\n",
    "    \"\"\"\n",
    "    Discretize quantity into bins for state representation in Q-learning.\n",
    "    Remember that qty is the predicted quantity sold. It is the target variable of demand model.\n",
    "    We need to make the state space finite for tabular Q-learning.\n",
    "    \"\"\"\n",
    "    # simple bins; tweak as desired:\n",
    "    # We can think of this as the following:\n",
    "    # q  < 1: no demand\n",
    "    # 1  < q < 10: very low demand\n",
    "    # 10 < q < 20: quite low demand\n",
    "    # 20 < q < 30: still low demand but better\n",
    "    # 30 < q < 40: low-medium demand\n",
    "    # 40 < q < 50: medium demand\n",
    "    # 50 < q < 100: high demand\n",
    "    # 100< q < 200: very high demand\n",
    "    # q > 200: extremely high demand\n",
    "    if q < 1: return 0\n",
    "    if q < 10: return 1\n",
    "    if q < 20: return 2\n",
    "    if q < 30: return 3\n",
    "    if q < 40: return 4\n",
    "    if q < 50: return 5\n",
    "    if q < 100: return 6\n",
    "    if q < 200: return 7\n",
    "    return 8\n",
    "\n",
    "# Using the demand model, predict quantity sold given a price and row features\n",
    "def predict_qty(model, row_dict, price):\n",
    "    \"\"\"\n",
    "    Predict quantity sold using the demand model.\n",
    "    row_dict contains features like qty_lag1, qty_lag7, n_txn, dow, month\n",
    "    Remark: dow and month are integers which will be one-hot encoded by the model\n",
    "    \"\"\"\n",
    "    # Ensure price is positive and reasonable\n",
    "    price = max(1e-6, float(price))\n",
    "    \n",
    "    # Check for invalid values in row_dict\n",
    "    qty_lag1 = max(0.0, float(row_dict[\"qty_lag1\"]))\n",
    "    qty_lag7 = max(0.0, float(row_dict[\"qty_lag7\"]))\n",
    "    n_txn = max(1.0, float(row_dict[\"n_txn\"]))\n",
    "    \n",
    "    x = {\n",
    "        \"log_price\": np.log(price),\n",
    "        \"qty_lag1\": qty_lag1,\n",
    "        \"qty_lag7\": qty_lag7,\n",
    "        \"n_txn\": n_txn,\n",
    "        \"dow\": int(row_dict[\"dow\"]),\n",
    "        \"month\": int(row_dict[\"month\"]),\n",
    "    }\n",
    "    X = pd.DataFrame([x])\n",
    "    \n",
    "    try:\n",
    "        log_qty = float(model.predict(X)[0])\n",
    "        # Clip log_qty to prevent extreme values\n",
    "        log_qty = np.clip(log_qty, -10, 10)\n",
    "        qty_hat = max(0.0, np.expm1(log_qty))\n",
    "    except:\n",
    "        qty_hat = 0.0\n",
    "    \n",
    "    return qty_hat\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) RL environment step\n",
    "# -----------------------------\n",
    "def env_step(model, row_dict, action_mult):\n",
    "    \"\"\"\n",
    "    One-step transition:\n",
    "      action_mult scales the base price\n",
    "      reward is revenue = price * predicted_qty\n",
    "      next_state uses qty_bin(predicted_qty)\n",
    "      we update lags naively for the simulated next step\n",
    "    \"\"\"\n",
    "    base_price = max(1e-6, float(row_dict[\"price\"]))\n",
    "    action_mult = max(0.1, min(2.0, float(action_mult)))  # Clamp action multiplier\n",
    "    new_price = max(1e-6, base_price * action_mult)\n",
    "\n",
    "    # Use demand-model to predict quantity sold at new_price\n",
    "    qty_hat = predict_qty(model, row_dict, new_price)\n",
    "    qty_hat = max(0.0, min(10000.0, qty_hat))  # Clamp quantity to reasonable range\n",
    "    \n",
    "    # compute reward using updated new_price\n",
    "    # so that the method will learn how to pick price multiplier that maximizes expected revenue\n",
    "    reward = new_price * qty_hat\n",
    "\n",
    "    # Note: the predicted qty_hat is continuous, so we need to bin them to create discrete state value for quantity sold\n",
    "    next_state = qty_bin(qty_hat)\n",
    "\n",
    "    # Build the next row (next \"state features\") -- We need to know what carries forward\n",
    "    # tomorrow's lag1 becomes today's predicted quantity\n",
    "    # tomorrow's lag7 become today's lag1\n",
    "    # Update lags for next step\n",
    "    next_row = dict(row_dict)\n",
    "    next_row[\"qty_lag7\"] = max(0.0, float(row_dict[\"qty_lag1\"]))\n",
    "    next_row[\"qty_lag1\"] = max(0.0, float(qty_hat))\n",
    "    next_row[\"price\"] = new_price\n",
    "    # Keep other context fields stable\n",
    "    next_row[\"n_txn\"] = max(1.0, float(row_dict.get(\"n_txn\", 1)))\n",
    "    next_row[\"dow\"] = int(row_dict[\"dow\"])\n",
    "    next_row[\"month\"] = int(row_dict[\"month\"])\n",
    "    # return Q-learning expects (signature variables)\n",
    "    return next_state, reward, next_row\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Q-learning with Advanced Updates\n",
    "# -----------------------------\n",
    "def q_learning(\n",
    "    ep_table: pd.DataFrame,\n",
    "    demand_model,\n",
    "    actions=(0.9, 1.0, 1.1),\n",
    "    episodes=5000,\n",
    "    horizon=20,\n",
    "    alpha=0.1,\n",
    "    gamma=0.9,\n",
    "    epsilon=0.25,\n",
    "    seed=0,\n",
    "    update_method=\"double_q\",  # \"bellman\", \"double_q\", \"expected_sarsa\", \"n_step\"\n",
    "    n_step=3,  # for n-step returns\n",
    "):\n",
    "    \"\"\"\n",
    "    Tabular Q-learning with multiple update methods.\n",
    "    \n",
    "    Update Methods:\n",
    "    - \"bellman\": Standard Q-learning (max over next state)\n",
    "    - \"double_q\": Double Q-Learning (reduces maximization bias)\n",
    "    - \"expected_sarsa\": Expected SARSA (uses expected value instead of max)\n",
    "    - \"n_step\": N-step Q-learning (uses multi-step returns)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    actions = np.array(list(actions), dtype=float)\n",
    "    n_states = 9\n",
    "    n_actions = len(actions)\n",
    "    \n",
    "    if update_method == \"double_q\":\n",
    "        # Double Q-Learning uses two Q-tables (2 separate estimates to reduce overestimation)\n",
    "        Q1 = np.zeros((n_states, n_actions), dtype=float)\n",
    "        Q2 = np.zeros((n_states, n_actions), dtype=float)\n",
    "    else:\n",
    "        Q = np.zeros((n_states, n_actions), dtype=float)\n",
    "\n",
    "    # Loop over episodes and randomly sampled starting points\n",
    "    # Goal: learn a pricing policy that maximizes EXPECTED long-term revenue across various products (states) and time periods\n",
    "    for _ in range(episodes):\n",
    "        # sample a random start row from ep_table \n",
    "        r = ep_table.iloc[int(rng.integers(0, len(ep_table)))].to_dict()\n",
    "        # initial state is based on lagged demand, not current demand\n",
    "        s = qty_bin(float(r[\"qty_lag1\"]))\n",
    "        \n",
    "        # For n-step, store trajectory\n",
    "        if update_method == \"n_step\":\n",
    "            trajectory = []\n",
    "\n",
    "        # Remark: we need to loop through the horizon steps because one episode in Q-learning can span multiple time steps\n",
    "        #         Think of it as simulating a sequence of decisions within one episode to capture temporal dynamics\n",
    "        #         Each step within the episode represents a decision point where the agent selects an action, observes the outcome, and updates its knowledge\n",
    "        #         This allows the agent to learn from a sequence of interactions rather than a single decision\n",
    "        #         Mind set: \"Today's price decision affects not just today's revenue but also future states and rewards\"\n",
    "        #         Goal: learn a pricing policy that maximizes long-term revenue over multiple time steps NOT just immediate reward (revenue)\n",
    "        # Mathematically, we are estimating:\n",
    "        # Q[s,a] ~ E[\n",
    "        #       sum_{t=0,...,infinity} \\gamma^t reward_t | s0=s, a0=a\n",
    "        # ]\n",
    "        # That expectation is over a trajectory of states and rewards aka transitions\n",
    "        # (s0,a0,reward0,s1) , (s1,a1,reward1,s2) , ...\n",
    "        # Rule of thumb: effective horizon is roughly 1/(1-gamma)\n",
    "        for t in range(horizon):\n",
    "            # epsilon-greedy action selection:\n",
    "            # given the current state s, available actions, current Q1,Q2 estimates\n",
    "            # decide: try something new? or use what I've learnt so far\n",
    "            #         with probability epsilon: explore (random action) and probability 1-epsilon: exploit (best known action)\n",
    "            if update_method == \"double_q\":\n",
    "                # Use average Q-values for action selection: this reduces variance, avoids trusting one noisy estimate, gives a more stable actions ranking\n",
    "                q_avg = (Q1[s] + Q2[s]) / 2\n",
    "                if rng.random() < epsilon:\n",
    "                    a = int(rng.integers(0, n_actions))     # explore other action choice randomly\n",
    "                else:\n",
    "                    a = int(np.argmax(q_avg))               # use best-action, determined by largest Q-value from Q_avg table\n",
    "            else:\n",
    "                if rng.random() < epsilon:\n",
    "                    a = int(rng.integers(0, n_actions))\n",
    "                else:\n",
    "                    a = int(np.argmax(Q[s]))\n",
    "            # once we have chosen action a in actions list, we can simulate step\n",
    "            # i.e., computing next state s2, reward and next row\n",
    "            s2, reward, r = env_step(demand_model, r, actions[a])\n",
    "\n",
    "            # ========== UPDATE RULES ==========\n",
    "            \n",
    "            if update_method == \"bellman\":\n",
    "                # Standard Bellman Q-Learning:\n",
    "                # Often overestimate due to using max() bias\n",
    "                Q[s, a] = Q[s, a] + alpha * (reward + gamma * np.max(Q[s2]) - Q[s, a])\n",
    "            \n",
    "            elif update_method == \"double_q\":\n",
    "                # Double Q-Learning: Randomly choose which Q to update\n",
    "                # Double Q-learning reduce maximization bias\n",
    "                if rng.random() < 0.5:\n",
    "                    # Update Q1, use Q2 for evaluation\n",
    "                    best_a = int(np.argmax(Q1[s2]))\n",
    "                    target = reward + gamma * Q2[s2, best_a]\n",
    "                    Q1[s, a] = Q1[s, a] + alpha * (target - Q1[s, a])\n",
    "                else:\n",
    "                    # Update Q2, use Q1 for evaluation\n",
    "                    best_a = int(np.argmax(Q2[s2]))\n",
    "                    target = reward + gamma * Q1[s2, best_a]\n",
    "                    Q2[s, a] = Q2[s, a] + alpha * (target - Q2[s, a])\n",
    "            \n",
    "            elif update_method == \"expected_sarsa\":\n",
    "                # Expected SARSA: Use expected value under current policy\n",
    "                # Compute probability distribution over actions in s2\n",
    "                if np.max(Q[s2]) > -1e10:  # if not all equal\n",
    "                    # Epsilon-greedy probabilities\n",
    "                    greedy_prob = 1.0 - epsilon + (epsilon / n_actions)\n",
    "                    explore_prob = epsilon / n_actions\n",
    "                    \n",
    "                    probs = np.full(n_actions, explore_prob)\n",
    "                    probs[np.argmax(Q[s2])] = greedy_prob\n",
    "                    \n",
    "                    expected_q = np.sum(probs * Q[s2])\n",
    "                else:\n",
    "                    expected_q = np.mean(Q[s2])\n",
    "                \n",
    "                Q[s, a] = Q[s, a] + alpha * (reward + gamma * expected_q - Q[s, a])\n",
    "            \n",
    "            elif update_method == \"n_step\":\n",
    "                # N-step Q-Learning: Store trajectory and update when we have n steps\n",
    "                trajectory.append((s, a, reward))\n",
    "                \n",
    "                if len(trajectory) >= n_step:\n",
    "                    # Compute n-step return\n",
    "                    G = 0.0\n",
    "                    for i, (s_i, a_i, r_i) in enumerate(trajectory[-n_step:]):\n",
    "                        G += (gamma ** i) * r_i\n",
    "                    \n",
    "                    # Add bootstrap value\n",
    "                    G += (gamma ** n_step) * np.max(Q[s2])\n",
    "                    \n",
    "                    # Update the state-action from n steps ago\n",
    "                    s_old, a_old, _ = trajectory[-n_step]\n",
    "                    Q[s_old, a_old] = Q[s_old, a_old] + alpha * (G - Q[s_old, a_old])\n",
    "            \n",
    "            # update state vector\n",
    "            s = s2\n",
    "        \n",
    "        # For n-step: update remaining states in trajectory\n",
    "        if update_method == \"n_step\" and len(trajectory) > 0:\n",
    "            for i in range(1, min(n_step, len(trajectory))):\n",
    "                # Compute remaining returns\n",
    "                G = 0.0\n",
    "                for j in range(len(trajectory) - i, len(trajectory)):\n",
    "                    s_j, a_j, r_j = trajectory[j]\n",
    "                    G += (gamma ** (j - (len(trajectory) - i))) * r_j\n",
    "                \n",
    "                s_old, a_old, _ = trajectory[-(i+1)]\n",
    "                Q[s_old, a_old] = Q[s_old, a_old] + alpha * (G - Q[s_old, a_old])\n",
    "\n",
    "    # Return the appropriate Q-table\n",
    "    if update_method == \"double_q\":\n",
    "        # Average the two Q-tables for final policy\n",
    "        Q = (Q1 + Q2) / 2\n",
    "    # Return Q = this is a table storing learned long-term value of taking each pricing action in each demand state\n",
    "    # Q[s,a] ~ E[\n",
    "    #       sum_{t=0,...,infinity} \\gamma^t reward_t | s0=s, a0=a, probability\n",
    "    # ]\n",
    "    # For example, Q can look like this\n",
    "    # state (demand bin)\t0.9×\t1.0×\t1.1×\n",
    "    # 0 (q < 1)\t            5.2\t    6.8\t    6.1\n",
    "    # 1 (1 ≤ q < 5)\t        12.1\t13.4\t12.9\n",
    "    # 2 (5 ≤ q < 10)\t    20.7\t22.9\t21.0\n",
    "    # 3 (10 ≤ q < 15)\t    33.0\t35.8\t34.2\n",
    "    # 4 (15 ≤ q < 20)\t    45.6\t47.1\t44.0\n",
    "    # 5 (q ≥ 20)\t        60.2\t61.0\t58.4\n",
    "    # So if the predicted demand is in bin (5 ≤ q < 10), and if we choose action (1.0x)\n",
    "    #    then the expected future revenue is ~ 22.9\n",
    "    # When the demand is in bin (5 ≤ q < 10), we want to choose action having highest Q-values, in this case action (1.0x)\n",
    "    return Q, actions\n",
    "\n",
    "\n",
    "# A more robust version of Q-learning with reward transformation, Temporal-Difference clipping, adaptive learning rates, and target smoothing\n",
    "def q_learning_robust(\n",
    "    ep_table: pd.DataFrame,\n",
    "    demand_model,\n",
    "    actions=(0.9, 1.0, 1.1),\n",
    "    episodes=5000,\n",
    "    horizon=20,\n",
    "    gamma=0.9,\n",
    "    seed=0,\n",
    "    update_method=\"double_q\",     # \"bellman\" or \"double_q\"\n",
    "    # exploration schedule\n",
    "    epsilon_start=0.5,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay=0.999,\n",
    "    # learning-rate schedule\n",
    "    alpha0=0.5,                   # base LR; actual lr = alpha0 / sqrt(N(s,a)+1)\n",
    "    # robustness knobs\n",
    "    reward_transform=\"log1p\",     # \"none\" or \"log1p\"\n",
    "    td_clip=50.0,                 # clip TD error to [-td_clip, td_clip]\n",
    "    # target smoothing (tabular \"target network\")\n",
    "    use_target=True,\n",
    "    tau=0.01,                     # Polyak rate for target tables\n",
    "    # state space\n",
    "    n_states=6                    # <-- set to match your qty_bin (0..5)\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    actions = np.array(list(actions), dtype=float)\n",
    "    n_actions = len(actions)\n",
    "\n",
    "    def transform_reward(r):\n",
    "        if reward_transform == \"log1p\":\n",
    "            return np.log1p(max(0.0, r))\n",
    "        return float(r)\n",
    "\n",
    "    # Visit counts for adaptive learning rates\n",
    "    N_sa = np.zeros((n_states, n_actions), dtype=np.int64)\n",
    "\n",
    "    if update_method == \"double_q\":\n",
    "        # initialize Double Q-learning tables\n",
    "        Q1 = np.zeros((n_states, n_actions), dtype=float)\n",
    "        Q2 = np.zeros((n_states, n_actions), dtype=float)\n",
    "\n",
    "        if use_target:\n",
    "            # Note: Q1_t and Q2_t are slow-moving target copies for Double Q-learning tables\n",
    "            #       They exist only to produce stable bootstrap targets.\n",
    "            Q1_t = Q1.copy()\n",
    "            Q2_t = Q2.copy()\n",
    "    else:\n",
    "        Q = np.zeros((n_states, n_actions), dtype=float)\n",
    "        if use_target:\n",
    "            Q_t = Q.copy()\n",
    "\n",
    "    eps = float(epsilon_start)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        # decay epsilon, using a factor epsilon_decay = 0.999, for each episode\n",
    "        eps = max(epsilon_end, eps * epsilon_decay)\n",
    "\n",
    "        # sample a start row\n",
    "        r = ep_table.iloc[int(rng.integers(0, len(ep_table)))].to_dict()\n",
    "        s = int(qty_bin(float(r[\"qty_lag1\"])))\n",
    "\n",
    "        for t in range(horizon):\n",
    "            # --- choose action (epsilon-greedy) ---\n",
    "            if update_method == \"double_q\":\n",
    "                q_beh = (Q1[s] + Q2[s]) / 2.0\n",
    "            else:\n",
    "                q_beh = Q[s]\n",
    "\n",
    "            if rng.random() < eps:\n",
    "                a = int(rng.integers(0, n_actions))\n",
    "            else:\n",
    "                a = int(np.argmax(q_beh))\n",
    "\n",
    "            # --- simulate env step ---\n",
    "            s2, reward, r = env_step(demand_model, r, actions[a])\n",
    "            s2 = int(s2)\n",
    "\n",
    "            # robust reward transformation (into log-scale). The goal is to stabilize learning by reducing variance in rewards\n",
    "            r_t = transform_reward(reward)\n",
    "\n",
    "            # adaptive learning rate for this (s,a)\n",
    "            # a classic robust technique: reduce LR for frequently-visited (s,a) pairs\n",
    "            # \\alpha(s,a) = alpha0 / sqrt(N(s,a)+1)\n",
    "            N_sa[s, a] += 1\n",
    "            alpha = alpha0 / np.sqrt(N_sa[s, a])\n",
    "\n",
    "            # ---------- compute bootstrap target ----------\n",
    "            # Note: In Q-learning, we bootstrap from our own estimates of future value\n",
    "            #       Bootstrap = targets are estimated using the model itself (not from actual observed rewards)\n",
    "            #       Standard Q-learning uses TD target: r_t + gamma * max_a Q(s_{t+1},a) (notice the RHS uses Q)\n",
    "            #       Double Q-learning uses TD target:   r_t + gamma * Q_other(s_{t+1}, argmax_a Q_self(s_{t+1},a))\n",
    "            #       This bootstrapping allows us to learn long-term value without needing full episode returns\n",
    "            if update_method == \"double_q\":\n",
    "                # Use target tables for bootstrap if enabled\n",
    "                if use_target:\n",
    "                    # We will use the slow-moving target tables for evaluation\n",
    "                    # We will NOT use it for choosing best actions (that would be too conservative) or for learning update.\n",
    "                    Q1_eval = Q1_t\n",
    "                    Q2_eval = Q2_t\n",
    "                else:\n",
    "                    Q1_eval = Q1\n",
    "                    Q2_eval = Q2\n",
    "\n",
    "                # Double-Q update (randomly update one table)\n",
    "                if rng.random() < 0.5:\n",
    "                    # select with Q1, evaluate with Q2\n",
    "                    # This is where we do bootstrapping: and the bootstrap term is gamma * Q2_eval[s2, best_a]\n",
    "                    # Bootstrapping mechanism:\n",
    "                    # - We select best action best_a using Q1 (the table we are updating)\n",
    "                    # - We evaluate that action using Q2_eval (the other table, possibly target table)\n",
    "                    best_a = int(np.argmax(Q1[s2]))\n",
    "                    target = r_t + gamma * Q2_eval[s2, best_a]  # evaluate with Q2 (bootstrap target)\n",
    "                    delta = target - Q1[s, a]                   # compute TD error: if delta>0, we underestimated Q1[s,a] --> increase it\n",
    "                                                                #                   if delta<0, we overestimated Q1[s,a]  --> decrease it\n",
    "                    # clip TD error to stabilize learning (TD error can be very large due to heavy-tailed rewards)\n",
    "                    if td_clip is not None:\n",
    "                        delta = float(np.clip(delta, -td_clip, td_clip))\n",
    "                    Q1[s, a] += alpha * delta\n",
    "                else:\n",
    "                    # select with Q2, evaluate with Q1 (similar reasoning as above)\n",
    "                    best_a = int(np.argmax(Q2[s2]))\n",
    "                    target = r_t + gamma * Q1_eval[s2, best_a]\n",
    "                    delta = target - Q2[s, a]\n",
    "                    if td_clip is not None:\n",
    "                        delta = float(np.clip(delta, -td_clip, td_clip))\n",
    "                    Q2[s, a] += alpha * delta\n",
    "\n",
    "            else:\n",
    "                # standard Q-learning\n",
    "                if use_target:\n",
    "                    bootstrap = np.max(Q_t[s2])\n",
    "                else:\n",
    "                    bootstrap = np.max(Q[s2])\n",
    "\n",
    "                target = r_t + gamma * bootstrap\n",
    "                delta = target - Q[s, a]\n",
    "                if td_clip is not None:\n",
    "                    delta = float(np.clip(delta, -td_clip, td_clip))\n",
    "                Q[s, a] += alpha * delta\n",
    "\n",
    "            # ---------- update targets (Polyak) ----------\n",
    "            # Polyak averaging to slowly update target tables towards current Q estimates\n",
    "            # It is a stability mechanism: by slowly tracking the learned Q-values, we reduce oscillations and divergence during learning\n",
    "            # In other words, it makes bootstrapped Q-learning behave much better in noisy environments.\n",
    "            # NOTE: Q-learning uses bootstrapped targets: target_t = r_t + gamma * Q(s_{t+1}, a)\n",
    "            #       So if Q changes quickly, the targets also change quickly --> instability\n",
    "            # Instead of bootstrapping from the latest Q, we use a smoothed version (target network)\n",
    "            # Q_target = (1 - tau) * Q_target + tau * Q where tau is a small value like 0.01 or even 0.001.\n",
    "            # Think of it as an exponential moving average of Q-values.\n",
    "            if use_target and (t % 1 == 0):  # update every step (cheap for tabular)\n",
    "                if update_method == \"double_q\":\n",
    "                    Q1_t = (1.0 - tau) * Q1_t + tau * Q1\n",
    "                    Q2_t = (1.0 - tau) * Q2_t + tau * Q2\n",
    "                else:\n",
    "                    Q_t = (1.0 - tau) * Q_t + tau * Q\n",
    "            # Another way to think of this is that if tau = 0.01 then we use 99% of old target and 1% of new Q-values\n",
    "            # Therefore, the target tables change very slowly over time, ensuring stability in learning\n",
    "\n",
    "            s = s2\n",
    "\n",
    "    if update_method == \"double_q\":\n",
    "        Q_out = (Q1 + Q2) / 2.0\n",
    "    else:\n",
    "        Q_out = Q\n",
    "\n",
    "    # Final Note:\n",
    "    # - Double-Q learning helps reduce overestimation bias in Q-values\n",
    "    # - Reward transformation (log1p) stabilizes learning by reducing variance in rewards\n",
    "    # - TD clipping prevents extreme updates that can destabilize learning\n",
    "    # - Adaptive learning rates ensure stable convergence by reducing step sizes for frequently-visited (s,a) pairs\n",
    "    # - Target smoothing (Polyak averaging) stabilizes bootstrapped targets i.e., fixing bootstrap instability.\n",
    "    return Q_out, actions\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Here is the core idea:\n",
    "# We are learning how actions change future states and how that affects long-term rewards\n",
    "# We are NOT learning one Q-value per state.\n",
    "# One episode represents a simulated short trajectory starting from a realistic state, sampled from historical data.\n",
    "# Formally, we are generating: (s0,a0,r0,s1), (s1,a1,r1,s2), ... , (s_{H-1}, a_{H-1}, r_{H-1}, s_H) where H = number of horizon steps\n",
    "# In very first step of horizon, we learn:\n",
    "# - If demand is in bin s0, and we take action a0 (price multiplier), then we get reward r0 and transition to state s1\n",
    "# In second step of horizon, we learn:\n",
    "# - If demand is in bin s1, and we take action a1, then we get reward r1 and transition to state s2\n",
    "# ...\n",
    "# These steps together update Q(s,a) values for multiple (s,a) pairs within one episode\n",
    "# Each TD (temporal-difference) update is:\n",
    "# Q[s,a] = Q[s,a] + alpha * (target - Q[s,a]) where target = r + gamma * max_a' Q[s',a']\n",
    "# This is done per horizon (not per episode)\n",
    "# TD learning philosophy: as soon as I see one step of experience, I update my belief (Q-values about that (s,a) pair)\n",
    "# So across one episode, we update:\n",
    "# Q[s0,a0] based on (r0, s1)\n",
    "# Q[s1,a1] based on (r1, s2)\n",
    "# Q[s2,a2] based on (r2, s3)\n",
    "# ...\n",
    "# In summary,\n",
    "# - During the horizon loop, the algorithm learns how today’s pricing action affects tomorrow’s demand state and future rewards, \n",
    "# allowing Q(s,a) to represent long-term value rather than just immediate revenue.\n",
    "# - Episode loop, by sampling different starting states from historical data, ensures diverse experience and robust learning across the state space.\n",
    "# I.e., episode loop teaches us about the EXPECTED long-term value of actions in various states.\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def extract_policy(Q, actions):\n",
    "    \"\"\"\n",
    "    Return mapping: state -> best action multiplier\n",
    "    policy[s] = argmax_a Q[s, a]\n",
    "    In particular, we want to choose the action having highest Q-value for demand s, which is bin_id for state\n",
    "    \"\"\"\n",
    "    best_a = np.argmax(Q, axis=1)\n",
    "    return {s: float(actions[a]) for s, a in enumerate(best_a)}\n",
    "\n",
    "\n",
    "def run_pipeline(top_k=200, seed=0, update_method=\"double_q\"):\n",
    "    \"\"\"\n",
    "    Run the full RL pricing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        top_k: Number of top products to include\n",
    "        seed: Random seed\n",
    "        update_method: Q-learning update rule (\"bellman\", \"double_q\", \"expected_sarsa\", \"n_step\")\n",
    "    \"\"\"\n",
    "    TOP_PRODUCTS = make_top_products(data, top_k=top_k)\n",
    "    episode_table = build_episode_table(data, TOP_PRODUCTS)\n",
    "    demand_model = fit_demand_model(episode_table)\n",
    "    # Q, actions = q_learning(\n",
    "    #     episode_table,\n",
    "    #     demand_model,\n",
    "    #     actions=ACTION_MULTS,\n",
    "    #     episodes=5000,\n",
    "    #     horizon=20,\n",
    "    #     alpha=0.1,\n",
    "    #     gamma=0.95,\n",
    "    #     epsilon=0.2,\n",
    "    #     seed=seed,\n",
    "    #     update_method=update_method,  # Use the specified update method\n",
    "    #     n_step=3,  # For n-step method\n",
    "    # )\n",
    "    Q, actions = q_learning_robust(\n",
    "        episode_table,\n",
    "        demand_model,\n",
    "        actions=ACTION_MULTS,\n",
    "        episodes=5000,\n",
    "        horizon=20,\n",
    "        gamma=0.95,\n",
    "        seed=seed,\n",
    "        update_method=update_method,    # Use the specified update method\n",
    "        epsilon_start=0.5,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay=0.9995,           # slower decay\n",
    "        alpha0=0.5,\n",
    "        reward_transform=\"log1p\",\n",
    "        td_clip=25.0,\n",
    "        use_target=True,\n",
    "        tau=0.01,\n",
    "        n_states=9,  # Match qty_bin\n",
    "    )\n",
    "    policy = extract_policy(Q, actions)\n",
    "    print(f\"\\n✓ Trained with update method: {update_method.upper()}\")\n",
    "    return policy, Q, actions, demand_model, episode_table\n",
    "\n",
    "# Train with Double Q-Learning (recommended - reduces overestimation bias)\n",
    "policy, Q, actions, demand_model, episode_table = run_pipeline(top_k=300, seed=42, update_method=\"double_q\")\n",
    "print(\"policy:\", policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1cc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned Q-table and policy\n",
    "print(\"=\" * 70)\n",
    "print(\"LEARNED Q-TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nQ-table shape: {Q.shape}\")\n",
    "print(f\"States: {Q.shape[0]} (quantity bins 0-5)\")\n",
    "print(f\"Actions: {Q.shape[1]} (price multipliers)\")\n",
    "print(f\"\\nPrice multipliers: {actions}\")\n",
    "print(f\"\\nQ-values:\")\n",
    "print(Q)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LEARNED POLICY\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nState (qty_bin) -> Best Price Multiplier:\")\n",
    "for state, action_mult in policy.items():\n",
    "    best_action_idx = np.argmax(Q[state])\n",
    "    q_value = Q[state, best_action_idx]\n",
    "    \n",
    "    if state == 0:\n",
    "        qty_range = \"qty < 1\"\n",
    "    elif state == 1:\n",
    "        qty_range = \"1 ≤ qty < 10\"\n",
    "    elif state == 2:\n",
    "        qty_range = \"10 ≤ qty < 20\"\n",
    "    elif state == 3:\n",
    "        qty_range = \"20 ≤ qty < 30\"\n",
    "    elif state == 4:\n",
    "        qty_range = \"30 ≤ qty < 40\"\n",
    "    elif state == 5:\n",
    "        qty_range = \"40 ≤ qty < 50\"\n",
    "    elif state == 6:\n",
    "        qty_range = \"50 ≤ qty < 100\"\n",
    "    elif state == 7:\n",
    "        qty_range = \"100 ≤ qty < 200\"\n",
    "    else:\n",
    "        qty_range = \"qty ≥ 200\"\n",
    "    \n",
    "    print(f\"  State {state} ({qty_range:15s}): price × {action_mult:.2f} (Q-value: {q_value:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"POLICY INSIGHTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count how many states prefer each action\n",
    "action_preferences = {}\n",
    "for state in range(Q.shape[0]):\n",
    "    best_action = np.argmax(Q[state])\n",
    "    mult = actions[best_action]\n",
    "    action_preferences[mult] = action_preferences.get(mult, 0) + 1\n",
    "\n",
    "print(\"\\nPrice multiplier distribution:\")\n",
    "for mult in sorted(action_preferences.keys()):\n",
    "    count = action_preferences[mult]\n",
    "    pct = 100 * count / Q.shape[0]\n",
    "    print(f\"  {mult:.2f}x: {count} states ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be705905",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "- Below evaluate_policy() uses un-paired t-test\n",
    "- This test assumes two samples are \"independent\", like:\n",
    "    - RL episodes are drawn from one population\n",
    "    - Baseline episodes are drawn from a different population\n",
    "- However, RL and baseline values are paired by construction:\n",
    "    - come from same row\n",
    "    - only action differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95706c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_policy(Q, actions, episode_table, demand_model, n_eval=1000, seed=42):\n",
    "#     \"\"\"\n",
    "#     Evaluate the learned pricing policy vs baseline strategies.\n",
    "    \n",
    "#     Args:\n",
    "#         Q: Learned Q-table\n",
    "#         actions: Array of action multipliers\n",
    "#         episode_table: DataFrame with episode data\n",
    "#         demand_model: Trained demand model\n",
    "#         n_eval: Number of evaluation episodes\n",
    "#         seed: Random seed for reproducibility\n",
    "    \n",
    "#     Returns:\n",
    "#         Dictionary with evaluation metrics\n",
    "#     \"\"\"\n",
    "#     rng = np.random.default_rng(seed)\n",
    "    \n",
    "#     # Storage for results\n",
    "#     rl_revenues = []\n",
    "#     baseline_revenues = []\n",
    "#     rl_quantities = []\n",
    "#     baseline_quantities = []\n",
    "#     rl_prices = []\n",
    "#     baseline_prices = []\n",
    "    \n",
    "#     for _ in range(n_eval):\n",
    "#         # Sample random starting state\n",
    "#         idx = int(rng.integers(0, len(episode_table)))\n",
    "#         row = episode_table.iloc[idx].to_dict()\n",
    "        \n",
    "#         # Get state\n",
    "#         state = qty_bin(float(row[\"qty_lag1\"]))\n",
    "#         base_price = float(row[\"price\"])\n",
    "        \n",
    "#         # RL Policy: use learned Q-table\n",
    "#         best_action = int(np.argmax(Q[state]))\n",
    "#         rl_mult = float(actions[best_action])\n",
    "#         rl_price = base_price * rl_mult\n",
    "#         rl_qty = predict_qty(demand_model, row, rl_price)\n",
    "#         rl_rev = rl_price * rl_qty\n",
    "        \n",
    "#         rl_revenues.append(rl_rev)\n",
    "#         rl_quantities.append(rl_qty)\n",
    "#         rl_prices.append(rl_price)\n",
    "        \n",
    "#         # Baseline: keep original price (1.0x multiplier)\n",
    "#         baseline_qty = predict_qty(demand_model, row, base_price)\n",
    "#         baseline_rev = base_price * baseline_qty\n",
    "        \n",
    "#         baseline_revenues.append(baseline_rev)\n",
    "#         baseline_quantities.append(baseline_qty)\n",
    "#         baseline_prices.append(base_price)\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     results = {\n",
    "#         'n_eval': n_eval,\n",
    "#         'rl_avg_revenue': np.mean(rl_revenues),\n",
    "#         'baseline_avg_revenue': np.mean(baseline_revenues),\n",
    "#         'rl_avg_quantity': np.mean(rl_quantities),\n",
    "#         'baseline_avg_quantity': np.mean(baseline_quantities),\n",
    "#         'rl_avg_price': np.mean(rl_prices),\n",
    "#         'baseline_avg_price': np.mean(baseline_prices),\n",
    "#         'rl_revenues': rl_revenues,\n",
    "#         'baseline_revenues': baseline_revenues,\n",
    "#         'revenue_improvement_pct': 100 * (np.mean(rl_revenues) - np.mean(baseline_revenues)) / np.mean(baseline_revenues),\n",
    "#         'rl_revenue_std': np.std(rl_revenues),\n",
    "#         'baseline_revenue_std': np.std(baseline_revenues),\n",
    "#     }\n",
    "    \n",
    "#     # Print evaluation results\n",
    "#     print(\"=\" * 70)\n",
    "#     print(\"POLICY EVALUATION RESULTS\")\n",
    "#     print(\"=\" * 70)\n",
    "#     print(f\"\\nEvaluated on {n_eval:,} random episodes\\n\")\n",
    "    \n",
    "#     print(\"REVENUE:\")\n",
    "#     print(f\"  Baseline (original prices): ${results['baseline_avg_revenue']:8.2f} (±${results['baseline_revenue_std']:.2f})\")\n",
    "#     print(f\"  RL Policy:                  ${results['rl_avg_revenue']:8.2f} (±${results['rl_revenue_std']:.2f})\")\n",
    "#     print(f\"  Improvement:                {results['revenue_improvement_pct']:+7.2f}%\")\n",
    "    \n",
    "#     print(\"\\nQUANTITY SOLD:\")\n",
    "#     print(f\"  Baseline:  {results['baseline_avg_quantity']:6.2f} units\")\n",
    "#     print(f\"  RL Policy: {results['rl_avg_quantity']:6.2f} units\")\n",
    "#     qty_change = 100 * (results['rl_avg_quantity'] - results['baseline_avg_quantity']) / results['baseline_avg_quantity']\n",
    "#     print(f\"  Change:    {qty_change:+6.2f}%\")\n",
    "    \n",
    "#     print(\"\\nAVERAGE PRICE:\")\n",
    "#     print(f\"  Baseline:  ${results['baseline_avg_price']:6.2f}\")\n",
    "#     print(f\"  RL Policy: ${results['rl_avg_price']:6.2f}\")\n",
    "#     price_change = 100 * (results['rl_avg_price'] - results['baseline_avg_price']) / results['baseline_avg_price']\n",
    "#     print(f\"  Change:    {price_change:+6.2f}%\")\n",
    "    \n",
    "#     # Statistical test\n",
    "#     from scipy import stats\n",
    "#     t_stat, p_value = stats.ttest_ind(rl_revenues, baseline_revenues)\n",
    "#     print(f\"\\nSTATISTICAL SIGNIFICANCE:\")\n",
    "#     print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "#     print(f\"  p-value:     {p_value:.6f}\")\n",
    "#     if p_value < 0.05:\n",
    "#         print(f\"  Result:      Statistically significant (p < 0.05) ✓\")\n",
    "#     else:\n",
    "#         print(f\"  Result:      Not statistically significant (p ≥ 0.05)\")\n",
    "    \n",
    "#     # Distribution analysis\n",
    "#     print(f\"\\nREVENUE DISTRIBUTION:\")\n",
    "#     print(f\"  Baseline - Min: ${np.min(baseline_revenues):.2f}, Max: ${np.max(baseline_revenues):.2f}, Median: ${np.median(baseline_revenues):.2f}\")\n",
    "#     print(f\"  RL Policy - Min: ${np.min(rl_revenues):.2f}, Max: ${np.max(rl_revenues):.2f}, Median: ${np.median(rl_revenues):.2f}\")\n",
    "    \n",
    "#     # Win rate\n",
    "#     wins = sum(1 for rl, bl in zip(rl_revenues, baseline_revenues) if rl > bl)\n",
    "#     win_rate = 100 * wins / n_eval\n",
    "#     print(f\"\\nWIN RATE:\")\n",
    "#     print(f\"  RL Policy beats baseline in {wins}/{n_eval} episodes ({win_rate:.1f}%)\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# # Run evaluation\n",
    "# eval_results = evaluate_policy(Q, actions, episode_table, demand_model, n_eval=20000, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf9e18",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "- Correct statistical test must be used is the Paired t-test\n",
    "- We also observed that quantity demand is heavily tailed --> revenue will also be heavily tailed by construction\n",
    "- In other words, revenue becomes skewed with outliers\n",
    "- A lession:\n",
    "    - classic t-test is sensitive to outlier, non-normalities, ...\n",
    "    - Need to use paired-testing & robust test for heavy tails (Wilcoxon signed-rank test)\n",
    "- Paired test: works because RL & baseline are computed from same row: they are not independent as assumed by unpaired test\n",
    "- Wilcoxon signed-rank test: more robust to heavy tails\n",
    "- Sign test/Binomial test (for win-rate): answer YES/NO to does RL beat baseline.\n",
    "    - So if p-value for this test ~ 0 ==> YES! RL beat baseline with statistically significant confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_policy(Q, actions, episode_table, demand_model, n_eval=5000, seed=420, do_bootstrap=True, n_boot=2000):\n",
    "    \"\"\"\n",
    "    Evaluate the learned pricing policy vs baseline strategies with robust statistical tests.\n",
    "\n",
    "    Adds:\n",
    "      - Paired t-test (ttest_rel)\n",
    "      - Wilcoxon signed-rank test (wilcoxon)\n",
    "      - Sign test via binomial test on win rate (binomtest)\n",
    "      - (Optional) Bootstrap CI for mean/median revenue lift\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    rl_revenues = []\n",
    "    baseline_revenues = []\n",
    "    rl_quantities = []\n",
    "    baseline_quantities = []\n",
    "    rl_prices = []\n",
    "    baseline_prices = []\n",
    "\n",
    "    for _ in range(n_eval):\n",
    "        # mimic of what we are doing in q_learning() function\n",
    "        idx = int(rng.integers(0, len(episode_table)))\n",
    "        row = episode_table.iloc[idx].to_dict()\n",
    "\n",
    "        state = qty_bin(float(row[\"qty_lag1\"])) # current state is demand quantity lag1\n",
    "        base_price = float(row[\"price\"])        # extract base price of current state\n",
    "\n",
    "        # RL Policy\n",
    "        best_action = int(np.argmax(Q[state]))  # for Q[s,a], we extract best a (action) for each discrete state\n",
    "        rl_mult = float(actions[best_action])   # extract RL's price multipliers according to best action\n",
    "        rl_price = base_price * rl_mult         # update RL price = base * multiplier-factor\n",
    "        rl_qty = predict_qty(demand_model, row, rl_price)   # using RL-price, predict demand\n",
    "        rl_rev = rl_price * rl_qty              # compute reward function using RL's recommended price and predicted demand quantity\n",
    "\n",
    "        rl_revenues.append(rl_rev)              # store revenues, predicted quantity and RL recommended price into vectors\n",
    "        rl_quantities.append(rl_qty)\n",
    "        rl_prices.append(rl_price)\n",
    "\n",
    "        # Baseline: baseline decision is basically using base_price without any multiplier (i.e. multiplier = 1x)\n",
    "        baseline_qty = predict_qty(demand_model, row, base_price)\n",
    "        baseline_rev = base_price * baseline_qty\n",
    "\n",
    "        baseline_revenues.append(baseline_rev)\n",
    "        baseline_quantities.append(baseline_qty)\n",
    "        baseline_prices.append(base_price)\n",
    "\n",
    "    rl_revenues = np.asarray(rl_revenues, dtype=float)\n",
    "    baseline_revenues = np.asarray(baseline_revenues, dtype=float)\n",
    "\n",
    "    # Core metrics\n",
    "    rl_mean = float(np.mean(rl_revenues))\n",
    "    bl_mean = float(np.mean(baseline_revenues))\n",
    "    rl_std = float(np.std(rl_revenues, ddof=1))\n",
    "    bl_std = float(np.std(baseline_revenues, ddof=1))\n",
    "\n",
    "    revenue_improvement_pct = 100.0 * (rl_mean - bl_mean) / (bl_mean + 1e-12)\n",
    "\n",
    "    results = {\n",
    "        \"n_eval\": n_eval,\n",
    "        \"rl_avg_revenue\": rl_mean,\n",
    "        \"baseline_avg_revenue\": bl_mean,\n",
    "        \"rl_avg_quantity\": float(np.mean(rl_quantities)),\n",
    "        \"baseline_avg_quantity\": float(np.mean(baseline_quantities)),\n",
    "        \"rl_avg_price\": float(np.mean(rl_prices)),\n",
    "        \"baseline_avg_price\": float(np.mean(baseline_prices)),\n",
    "        \"rl_revenue_std\": rl_std,\n",
    "        \"baseline_revenue_std\": bl_std,\n",
    "        \"revenue_improvement_pct\": revenue_improvement_pct,\n",
    "        \"rl_revenues\": rl_revenues,\n",
    "        \"baseline_revenues\": baseline_revenues,\n",
    "    }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Statistical tests\n",
    "    # -----------------------------\n",
    "    from scipy import stats\n",
    "\n",
    "    diff = rl_revenues - baseline_revenues  # compute difference between RL's reward and baseline's reward functions\n",
    "\n",
    "    # 1) Paired t-test (recommended)\n",
    "    t_rel, p_rel = stats.ttest_rel(rl_revenues, baseline_revenues)\n",
    "\n",
    "    # 2) Wilcoxon signed-rank test (paired, nonparametric)\n",
    "    # Wilcoxon requires non-all-zero diffs; handle edge case safely\n",
    "    if np.allclose(diff, 0.0):\n",
    "        w_stat, p_wil = np.nan, 1.0\n",
    "    else:\n",
    "        # zero_method='wilcox' ignores zero diffs; alternative='greater' tests RL > baseline\n",
    "        try:\n",
    "            w_stat, p_wil = stats.wilcoxon(diff, zero_method=\"wilcox\", alternative=\"greater\")\n",
    "        except TypeError:\n",
    "            # Older SciPy may not support 'alternative'; fall back to two-sided\n",
    "            w_stat, p_wil = stats.wilcoxon(diff, zero_method=\"wilcox\")\n",
    "\n",
    "    # 3) Sign test via Binomial test on win-rate (paired, very robust)\n",
    "    # this can be done by calculate number of wins: a win is defined by positive difference in (rl - baseline) revenues\n",
    "    wins = int(np.sum(diff > 0))\n",
    "    ties = int(np.sum(diff == 0))\n",
    "    # Exclude ties for the sign test (standard approach)\n",
    "    n_eff = n_eval - ties\n",
    "    if n_eff > 0:\n",
    "        p_sign = stats.binomtest(wins, n_eff, 0.5, alternative=\"greater\").pvalue\n",
    "    else:\n",
    "        p_sign = 1.0\n",
    "\n",
    "    # 4) (Optional) Bootstrap CI for mean/median lift (paired resampling)\n",
    "    boot_ci = None\n",
    "    if do_bootstrap:\n",
    "        idxs = np.arange(n_eval)\n",
    "        boot_means = np.empty(n_boot, dtype=float)\n",
    "        boot_medians = np.empty(n_boot, dtype=float)\n",
    "        for b in range(n_boot):\n",
    "            samp = rng.choice(idxs, size=n_eval, replace=True)\n",
    "            d = diff[samp]\n",
    "            boot_means[b] = np.mean(d)\n",
    "            boot_medians[b] = np.median(d)\n",
    "\n",
    "        boot_ci = {\n",
    "            \"mean_diff\": float(np.mean(diff)),\n",
    "            \"mean_diff_ci95\": (float(np.quantile(boot_means, 0.025)), float(np.quantile(boot_means, 0.975))),\n",
    "            \"median_diff\": float(np.median(diff)),\n",
    "            \"median_diff_ci95\": (float(np.quantile(boot_medians, 0.025)), float(np.quantile(boot_medians, 0.975))),\n",
    "        }\n",
    "\n",
    "    # Save stats in results\n",
    "    results[\"stats\"] = {\n",
    "        \"paired_ttest\": {\"t\": float(t_rel), \"p\": float(p_rel)},\n",
    "        \"wilcoxon\": {\"stat\": float(w_stat) if not np.isnan(w_stat) else np.nan, \"p\": float(p_wil)},\n",
    "        \"sign_test_binom\": {\"wins\": wins, \"ties\": ties, \"n_eff\": n_eff, \"p\": float(p_sign)},\n",
    "        \"bootstrap_ci\": boot_ci,\n",
    "    }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Print results\n",
    "    # -----------------------------\n",
    "    print(\"=\" * 70)\n",
    "    print(\"POLICY EVALUATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nEvaluated on {n_eval:,} random episodes\\n\")\n",
    "\n",
    "    print(\"REVENUE:\")\n",
    "    print(f\"  Baseline (original prices): ${bl_mean:8.2f} (±${bl_std:,.2f})\")\n",
    "    print(f\"  RL Policy:                  ${rl_mean:8.2f} (±${rl_std:,.2f})\")\n",
    "    print(f\"  Improvement:                {revenue_improvement_pct:+7.2f}%\")\n",
    "\n",
    "    print(\"\\nQUANTITY SOLD:\")\n",
    "    print(f\"  Baseline:  {results['baseline_avg_quantity']:6.2f} units\")\n",
    "    print(f\"  RL Policy: {results['rl_avg_quantity']:6.2f} units\")\n",
    "    qty_change = 100 * (results[\"rl_avg_quantity\"] - results[\"baseline_avg_quantity\"]) / (results[\"baseline_avg_quantity\"] + 1e-12)\n",
    "    print(f\"  Change:    {qty_change:+6.2f}%\")\n",
    "\n",
    "    print(\"\\nAVERAGE PRICE:\")\n",
    "    print(f\"  Baseline:  ${results['baseline_avg_price']:6.2f}\")\n",
    "    print(f\"  RL Policy: ${results['rl_avg_price']:6.2f}\")\n",
    "    price_change = 100 * (results[\"rl_avg_price\"] - results[\"baseline_avg_price\"]) / (results[\"baseline_avg_price\"] + 1e-12)\n",
    "    print(f\"  Change:    {price_change:+6.2f}%\")\n",
    "\n",
    "    print(\"\\nSTATISTICAL SIGNIFICANCE (recommended for your setup):\")\n",
    "    print(f\"  Paired t-test (mean diff):      t={t_rel:.3f}, p={p_rel:.6f}\")\n",
    "    print(f\"  Wilcoxon (median/robust):       stat={w_stat:.3f}, p={p_wil:.6f}\")\n",
    "    print(f\"  Sign test (win-rate, robust):   wins={wins}/{n_eff} (ties={ties}), p={p_sign:.6g}\")\n",
    "\n",
    "    if do_bootstrap and boot_ci is not None:\n",
    "        lo, hi = boot_ci[\"mean_diff_ci95\"]\n",
    "        mlo, mhi = boot_ci[\"median_diff_ci95\"]\n",
    "        print(\"\\nBOOTSTRAP 95% CI (paired):\")\n",
    "        print(f\"  Mean revenue lift:   {boot_ci['mean_diff']:.3f}  CI[{lo:.3f}, {hi:.3f}]\")\n",
    "        print(f\"  Median revenue lift: {boot_ci['median_diff']:.3f}  CI[{mlo:.3f}, {mhi:.3f}]\")\n",
    "\n",
    "    print(\"\\nREVENUE DISTRIBUTION:\")\n",
    "    print(f\"  Baseline - Min: ${np.min(baseline_revenues):.2f}, Max: ${np.max(baseline_revenues):.2f}, Median: ${np.median(baseline_revenues):.2f}\")\n",
    "    print(f\"  RL Policy - Min: ${np.min(rl_revenues):.2f}, Max: ${np.max(rl_revenues):.2f}, Median: ${np.median(rl_revenues):.2f}\")\n",
    "\n",
    "    win_rate = 100.0 * wins / max(1, n_eff)\n",
    "    print(\"\\nWIN RATE:\")\n",
    "    print(f\"  RL Policy beats baseline in {wins}/{n_eff} episodes ({win_rate:.1f}%)\")\n",
    "\n",
    "    return results\n",
    "\n",
    "eval_results = evaluate_policy(Q, actions, episode_table, demand_model, n_eval=5000, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149359cc",
   "metadata": {},
   "source": [
    "### Visualize Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of evaluation results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Revenue Comparison Histogram\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(eval_results['baseline_revenues'], bins=100, alpha=0.6, label='Baseline', color='blue', edgecolor='black')\n",
    "ax1.hist(eval_results['rl_revenues'], bins=100, alpha=0.6, label='RL Policy', color='green', edgecolor='black')\n",
    "ax1.axvline(eval_results['baseline_avg_revenue'], color='blue', linestyle='--', linewidth=2, label=f'Baseline Avg: ${eval_results[\"baseline_avg_revenue\"]:.2f}')\n",
    "ax1.axvline(eval_results['rl_avg_revenue'], color='green', linestyle='--', linewidth=2, label=f'RL Avg: ${eval_results[\"rl_avg_revenue\"]:.2f}')\n",
    "ax1.set_xlim([0.0, 2000.0])\n",
    "ax1.set_xlabel('Revenue ($)', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Revenue Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Scatter plot: RL vs Baseline Revenue\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(eval_results['baseline_revenues'], eval_results['rl_revenues'], alpha=0.5, s=20)\n",
    "max_rev = max(max(eval_results['baseline_revenues']), max(eval_results['rl_revenues']))\n",
    "min_rev = min(min(eval_results['baseline_revenues']), min(eval_results['rl_revenues']))\n",
    "ax2.plot([min_rev, max_rev], [min_rev, max_rev], 'r--', linewidth=2, label='Equal Revenue Line')\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xlabel('Baseline Revenue ($)', fontsize=12)\n",
    "ax2.set_ylabel('RL Policy Revenue ($)', fontsize=12)\n",
    "ax2.set_title('RL vs Baseline Revenue (per episode)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Add text showing win rate\n",
    "effective_win_rate = 100.0 * eval_results['stats']['sign_test_binom']['wins'] / max(1, eval_results['stats']['sign_test_binom']['n_eff'])\n",
    "ax2.text(0.05, 0.95, f'Effective RL Wins (Excluding Ties): {effective_win_rate:.1f}%', transform=ax2.transAxes, \n",
    "         fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 3. Q-value heatmap\n",
    "ax3 = axes[1, 0]\n",
    "im = ax3.imshow(Q, aspect='auto', cmap='RdYlGn', interpolation='nearest')\n",
    "ax3.set_xlabel('Action (Price Multiplier)', fontsize=12)\n",
    "ax3.set_ylabel('State (Quantity Bin)', fontsize=12)\n",
    "ax3.set_title('Learned Q-Values Heatmap', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(range(len(actions)))\n",
    "ax3.set_xticklabels([f'{a:.2f}x' for a in actions])\n",
    "ax3.set_yticks(range(Q.shape[0]))\n",
    "ax3.set_yticklabels([f'State {i}' for i in range(Q.shape[0])])\n",
    "plt.colorbar(im, ax=ax3, label='Q-value')\n",
    "\n",
    "# Add annotations for best actions\n",
    "for i in range(Q.shape[0]):\n",
    "    for j in range(Q.shape[1]):\n",
    "        text = ax3.text(j, i, f'{Q[i, j]:.1f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "# 4. Bar chart: Average metrics comparison\n",
    "ax4 = axes[1, 1]\n",
    "metrics = ['Revenue', 'Quantity', 'Price']\n",
    "baseline_vals = [\n",
    "    eval_results['baseline_avg_revenue'],\n",
    "    eval_results['baseline_avg_quantity'],\n",
    "    eval_results['baseline_avg_price']\n",
    "]\n",
    "rl_vals = [\n",
    "    eval_results['rl_avg_revenue'],\n",
    "    eval_results['rl_avg_quantity'],\n",
    "    eval_results['rl_avg_price']\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, baseline_vals, width, label='Baseline', color='blue', alpha=0.7)\n",
    "bars2 = ax4.bar(x + width/2, rl_vals, width, label='RL Policy', color='green', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Metric', fontsize=12)\n",
    "ax4.set_ylabel('Value', fontsize=12)\n",
    "ax4.set_title('Average Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics)\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VISUALIZATION COMPLETE\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37ddd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
